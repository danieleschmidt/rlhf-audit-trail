# pytest-benchmark configuration for RLHF Audit Trail

[tool:pytest-benchmark]
# Only run benchmarks when explicitly requested
only_run_when_requested = true

# Minimum rounds and iterations for stable results
min_rounds = 5
min_time = 0.000005

# Maximum time per benchmark (in seconds)
max_time = 10.0

# Number of warmup iterations
warmup = true
warmup_iterations = 3

# Disable GC during benchmarks for more stable results
disable_gc = true

# Sort results by mean time
sort = mean

# Compare benchmarks automatically
compare = mean

# Save benchmark results
autosave = true
save = benchmarks/results/pytest_benchmarks.json

# Histogram settings
histogram = true

# Machine info in results
machine_info = true

# Calibration settings
calibration_precision = 10
calibration_bias = 1e-9